# -*- coding: utf-8 -*-
"""230413612_Programming_File.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18LMeYL0yfQYiWhA6AFZX-RT_K-HtCgW-

#Financial Forecasting: Interactive Machine Learning Models for Data-Driven Predictions

#### Kabilan Mani
#### 230413612
#### ec230413612@qmul.ac.uk

The datasets were generated using a separate Python script, and the resulting files are included in the supporting documentation.

### 1. Importing Essential Libraries

Here, we're bringing together all the tools and libraries that will empower our data analysis and model-building process. Each library has a specific role, from basic data manipulation with **numpy** and **pandas** to advanced machine learning with **xgboost** and **pytorch**.
"""

# Importing necessary libraries for data manipulation, machine learning, and deep learning.
# numpy and pandas are used for numerical operations and data handling.
# sklearn provides tools for data preprocessing, model selection, and evaluation.
# xgboost is a popular library for gradient boosting algorithms.
# torch is used for building and training deep learning models.
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import xgboost as xgb
from sklearn.feature_selection import SelectKBest, f_regression
from sklearn.neural_network import MLPRegressor
import torch
from torch import nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from statsmodels.tsa.arima.model import ARIMA

"""### 2. Loading and Merging Datasets

We're loading two key datasets, ensuring that their formats align, and then merging them based on the date. This merge gives us a powerful dataset where **financial indicators** and **stock performance** are side by side, laying the groundwork for our predictive models.
"""

# Load financial and stock data
merged_financial_data = pd.read_csv('Merged_Financial_Data.csv')
stock_data = pd.read_csv('TSLA_Quarterly_Data.csv')

# Ensure column names are consistent
merged_financial_data.columns = merged_financial_data.columns.astype(str)
stock_data.columns = stock_data.columns.astype(str)

# Convert 'Date' columns to datetime format
merged_financial_data['Date'] = pd.to_datetime(merged_financial_data['Date'])
stock_data['Date'] = pd.to_datetime(stock_data['Date'])

# Merge datasets on 'Date'
merged_df = pd.merge(merged_financial_data, stock_data, on='Date', how='inner')

# Prepare features and target
features = merged_df.drop(columns=['Date', 'Close_y'])
target = merged_df['Close_y']

# Standardize features
scaler = StandardScaler()
scaled_features = scaler.fit_transform(features)

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(scaled_features, target, test_size=0.2, random_state=42)

# Impute missing values with the mean
imputer = SimpleImputer(strategy='mean')
X_train_imputed = imputer.fit_transform(X_train)
X_test_imputed = imputer.transform(X_test)

"""### 3. Preparing Features and Scaling

After merging the datasets, we prepare our feature matrix and target variable. Scaling the features ensures that each one contributes equally to the model, and splitting the data allows us to assess how well our model generalizes to unseen data.
"""

# Preparing the feature matrix (X) by dropping non-essential columns and setting the stock price as the target variable (y).
features = merged_df.drop(columns=['Date', 'Close_y'])
target = merged_df['Close_y']

# Standardizing the feature matrix using StandardScaler.
# This step is crucial for many machine learning algorithms, which perform better when features are on a similar scale.
scaler = StandardScaler()
scaled_features = scaler.fit_transform(features)

# Splitting the dataset into training and testing sets to evaluate our model's performance.
# We use 80% of the data for training and reserve 20% for testing.
X_train, X_test, y_train, y_test = train_test_split(scaled_features, target, test_size=0.2, random_state=42)

"""### 4. Handling Missing Data

Missing data can be a common issue in real-world datasets. Here, we're using a straightforward imputation technique to fill in these gaps, ensuring that our model can work with a complete dataset.
"""

# Addressing any missing values in the dataset by imputing them with the mean of each feature.
# This is a simple yet effective way to handle missing data without losing any rows from our dataset.
imputer = SimpleImputer(strategy='mean')
X_train_imputed = imputer.fit_transform(X_train)
X_test_imputed = imputer.transform(X_test)

"""### 5.Model

#### 5.1 ARIMA Model Training

This code provides a comprehensive approach to building and evaluating an ARIMA model for time series forecasting. It first ensures the data is stationary, applies differencing if needed, and optionally tunes the ARIMA parameters using auto_arima.
This workflow is typical in time series analysis and helps in creating reliable forecasting models.
"""

import numpy as np
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from statsmodels.tsa.arima.model import ARIMA

# Ensure the index is reset before fitting the ARIMA model
y_train_diff = pd.Series(y_train_diff).reset_index(drop=True)

# ARIMA model setup
arima_model = ARIMA(y_train_diff, order=(5, 1, 0))
arima_result = arima_model.fit()

# Forecasting
arima_forecast_diff = arima_result.forecast(steps=len(y_test))
arima_forecast_diff = pd.Series(arima_forecast_diff).reset_index(drop=True)

# Reverse differencing to get the forecast back to the original scale
last_value = y_train.iloc[-1] if len(y_train_diff) < len(y_train) else y_train_diff.iloc[-1]
arima_forecast = np.r_[last_value, arima_forecast_diff].cumsum()[1:]

# Ensure indices match for evaluation
arima_forecast = pd.Series(arima_forecast, index=y_test.index)

# Load and transpose data for feature selection
data_path = 'Final_Transposed_Financial_Data_with_Category.csv'
df = pd.read_csv(data_path)
df_transposed = df.set_index('category').T

# Extract features and target
target_column = 'Close'
features = df_transposed.drop(columns=[target_column]).values
target = df_transposed[target_column].values

# Handle missing values
features = pd.DataFrame(features).fillna(features.mean()).values
target = np.nan_to_num(target)

# Scale features
scaler = StandardScaler()
features_scaled = scaler.fit_transform(features)

# Split data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(features_scaled, target, test_size=0.2, random_state=42)

# Convert data to PyTorch tensors
X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)
X_val_tensor = torch.tensor(X_val, dtype=torch.float32)
y_val_tensor = torch.tensor(y_val, dtype=torch.float32).view(-1, 1)

"""### 5.2 XGBoost Model Training

XGBoost is a powerful tool for regression tasks. By training it on our data, we're creating a model that can learn the intricate patterns between financial metrics and stock prices, setting the stage for our predictions.
"""

import xgboost as xgb
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

# Check the shapes of the training data and labels
print(f'Shape of X_train_imputed: {X_train_imputed.shape}')
print(f'Shape of y_train: {y_train.shape}')
print(f'Shape of X_test_imputed: {X_test_imputed.shape}')
print(f'Shape of y_test: {y_test.shape}')

# Ensure that X_train and y_train have the same number of rows
min_train_len = min(X_train_imputed.shape[0], y_train.shape[0])
X_train_imputed = X_train_imputed[:min_train_len]
y_train = y_train[:min_train_len]

# Ensure there are no missing values in y_train and y_test
print(f'Missing values in y_train: {np.isnan(y_train).sum()}')
print(f'Missing values in y_test: {np.isnan(y_test).sum()}')

# Prepare data for XGBoost
try:
    dtrain = xgb.DMatrix(X_train_imputed, label=y_train)
    dtest = xgb.DMatrix(X_test_imputed, label=y_test)

    # Define and train XGBoost model
    params = {
        'objective': 'reg:squarederror',
        'max_depth': 7,
        'learning_rate': 0.01,
        'n_estimators': 100,
        'verbosity': 1
    }
    bst = xgb.train(params, dtrain, num_boost_round=100)

    # Make predictions
    y_test_pred_xgb = bst.predict(dtest)

    # Evaluate XGBoost model
    xgb_mse = mean_squared_error(y_test, y_test_pred_xgb)
    xgb_mae = mean_absolute_error(y_test, y_test_pred_xgb)
    xgb_r2 = r2_score(y_test, y_test_pred_xgb)

except xgb.core.XGBoostError as e:
    print(f'XGBoost Error: {e}')

"""#### 5.3 Hybrid Model (ARIMA + Neural Network) Training

This code implements a hybrid model combining the strengths of ARIMA and a neural network. The ARIMA model captures linear trends and patterns in the time series, while the neural network learns any remaining non-linear patterns from the residuals.
1. By training on these residuals, the network helps improve the overall forecasting accuracy, making it a powerful approach for time series prediction tasks.
2. The inclusion of early stopping and learning rate scheduling ensures that the model trains efficiently and avoids overfitting.
"""

import torch
from torch import nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
from torch.optim.lr_scheduler import ReduceLROnPlateau

# Truncate y_train and ARIMA fitted values to have the same length
min_len = min(len(y_train), len(arima_result.fittedvalues))
y_train_truncated = y_train[:min_len]
arima_fitted_truncated = arima_result.fittedvalues[:min_len]

# Calculate residuals from ARIMA model
arima_residuals = y_train_truncated - arima_fitted_truncated

# Split data into training and validation sets for better evaluation
residual_train, residual_val = train_test_split(arima_residuals, test_size=0.2, random_state=42)

# Prepare residual data for Neural Network
residual_train_tensor = torch.tensor(residual_train.values, dtype=torch.float32).view(-1, 1)
residual_val_tensor = torch.tensor(residual_val.values, dtype=torch.float32).view(-1, 1)

# Define a more complex neural network model
class ResidualNN(nn.Module):
    def __init__(self):
        super(ResidualNN, self).__init__()
        self.fc1 = nn.Linear(residual_train_tensor.shape[1], 256)
        self.fc2 = nn.Linear(256, 128)
        self.fc3 = nn.Linear(128, 64)
        self.fc4 = nn.Linear(64, 32)
        self.fc5 = nn.Linear(32, 1)
        self.dropout = nn.Dropout(0.3)
        self.bn1 = nn.BatchNorm1d(256)
        self.bn2 = nn.BatchNorm1d(128)
        self.bn3 = nn.BatchNorm1d(64)
        self.bn4 = nn.BatchNorm1d(32)

    def forward(self, x):
        x = torch.relu(self.bn1(self.fc1(x)))
        x = torch.relu(self.bn2(self.fc2(x)))
        x = self.dropout(x)
        x = torch.relu(self.bn3(self.fc3(x)))
        x = torch.relu(self.bn4(self.fc4(x)))
        x = self.fc5(x)
        return x

# Initialize the model, criterion, and optimizer
nn_model = ResidualNN()
criterion = nn.MSELoss()
optimizer = optim.Adam(nn_model.parameters(), lr=0.001)
scheduler = ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5, verbose=True)

# Training with early stopping, learning rate scheduling, and validation
num_epochs = 100
best_val_loss = float('inf')
patience, trials = 10, 0

for epoch in range(num_epochs):
    nn_model.train()
    optimizer.zero_grad()
    y_pred = nn_model(residual_train_tensor)
    loss = criterion(y_pred, residual_train_tensor)
    loss.backward()
    optimizer.step()

    nn_model.eval()
    with torch.no_grad():
        val_pred = nn_model(residual_val_tensor)
        val_loss = criterion(val_pred, residual_val_tensor)

    scheduler.step(val_loss)

    if val_loss < best_val_loss:
        best_val_loss = val_loss
        trials = 0
    else:
        trials += 1
        if trials >= patience:
            print(f"Early stopping on epoch {epoch+1}")
            break

    print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {loss.item():.4f}, Validation Loss: {val_loss.item():.4f}')

# After training, use the model for predictions or further evaluation

# Predict residuals for the validation set
nn_model.eval()
with torch.no_grad():
    predicted_residuals = nn_model(residual_val_tensor)

# Add the predicted residuals back to the ARIMA forecast to get the final prediction
final_predictions = arima_fitted_truncated[-len(predicted_residuals):] + predicted_residuals.numpy().flatten()

"""After training the model, the below section forecasts future values and evaluates the model’s accuracy using the MSE metric.

### 6.Evaluation of Models

This code segment evaluates three different models on a test dataset:

    **ARIMA Model**: A traditional time series model that captures linear patterns.
    **XGBoost Model**: A powerful machine learning model that captures complex patterns and interactions.
    **Hybrid Model**: A combination of ARIMA and a neural network, designed to capture both linear and non-linear patterns in the data.
Each model's performance is measured using standard metrics like MSE, MAE, and R2 Score.
"""

import numpy as np
import torch
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# Evaluate ARIMA model
arima_mse = mean_squared_error(y_test, arima_forecast)
arima_mae = mean_absolute_error(y_test, arima_forecast)
arima_r2 = r2_score(y_test, arima_forecast)

# Print the evaluation results
print(f'ARIMA Model MSE: {arima_mse:.4f}')
print(f'ARIMA Model MAE: {arima_mae:.4f}')
print(f'ARIMA Model R2 Score: {arima_r2:.4f}')

#Evaluate XGBoost Model

print(f'XGBoost Model MSE: {xgb_mse:.4f}')
print(f'XGBoost Model MAE: {xgb_mae:.4f}')
print(f'XGBoost Model R2 Score: {xgb_r2:.4f}')


# Evaluate the final predictions
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

mse = mean_squared_error(y_train_truncated[-len(final_predictions):], final_predictions)
mae = mean_absolute_error(y_train_truncated[-len(final_predictions):], final_predictions)
r2 = r2_score(y_train_truncated[-len(final_predictions):], final_predictions)

print(f"Hybrid Model Performance:\nMSE: {mse:.4f}\nMAE: {mae:.4f}\nR2 Score: {r2:.4f}")

pip install pmdarima

"""### Fine Tuning Models"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV, TimeSeriesSplit
from sklearn.impute import SimpleImputer
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import xgboost as xgb
import torch
from torch import nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.seasonal import seasonal_decompose
from pmdarima import auto_arima
from statsmodels.tsa.stattools import adfuller

# Load financial and stock data
merged_financial_data = pd.read_csv('Merged_Financial_Data.csv')
stock_data = pd.read_csv('TSLA_Quarterly_Data.csv')

# Ensure column names are consistent
merged_financial_data.columns = merged_financial_data.columns.astype(str)
stock_data.columns = stock_data.columns.astype(str)

# Convert 'Date' columns to datetime format
merged_financial_data['Date'] = pd.to_datetime(merged_financial_data['Date'])
stock_data['Date'] = pd.to_datetime(stock_data['Date'])

# Merge datasets on 'Date'
merged_df = pd.merge(merged_financial_data, stock_data, on='Date', how='inner')

# Prepare features and target
features = merged_df.drop(columns=['Date', 'Close_y'])
target = merged_df['Close_y']

# Standardize features
scaler = StandardScaler()
scaled_features = scaler.fit_transform(features)

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(scaled_features, target, test_size=0.2, random_state=42)

# Impute missing values with the mean
imputer = SimpleImputer(strategy='mean')
X_train_imputed = imputer.fit_transform(X_train)
X_test_imputed = imputer.transform(X_test)

# Function to test stationarity and return p-value
def test_stationarity(timeseries):
    result = adfuller(timeseries)
    print(f'ADF Statistic: {result[0]}')
    print(f'p-value: {result[1]}')
    for key, value in result[4].items():
        print(f'Critical Values {key}: {value}')
    return result[1]  # Return the p-value

# Check if the time series is stationary
p_value = test_stationarity(y_train)

# Use auto_arima to find the best parameters for ARIMA
arima_model = auto_arima(y_train, seasonal=False, trace=True)
arima_order = arima_model.order
arima_model = ARIMA(y_train, order=arima_order).fit()

# Forecast using ARIMA
arima_forecast = arima_model.predict(start=len(y_train), end=len(y_train)+len(y_test)-1, dynamic=False)

# Evaluate ARIMA model
arima_mse = mean_squared_error(y_test, arima_forecast)
arima_mae = mean_absolute_error(y_test, arima_forecast)
arima_r2 = r2_score(y_test, arima_forecast)

from sklearn.model_selection import TimeSeriesSplit

# TimeSeriesSplit allows you to split time series data with respect to time order
tscv = TimeSeriesSplit(n_splits=5)

# Ensure y_train is a numpy array to allow indexing
y_train_array = y_train.to_numpy()

for train_index, test_index in tscv.split(X_train):
    X_train_cv, X_test_cv = X_train[train_index], X_train[test_index]
    y_train_cv, y_test_cv = y_train_array[train_index], y_train_array[test_index]

# XGBoost model training with time series split
param_grid = {
    'max_depth': [4, 6, 8],
    'learning_rate': [0.01, 0.05, 0.1],
    'n_estimators': [100, 200],
    'subsample': [0.8, 1.0],
    'colsample_bytree': [0.8, 1.0]
}
xgb_model = xgb.XGBRegressor(objective='reg:squarederror')
grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=tscv, scoring='neg_mean_squared_error', n_jobs=-1)
grid_search.fit(X_train_imputed, y_train)
best_xgb_model = grid_search.best_estimator_

# Make predictions with XGBoost
y_test_pred_xgb = best_xgb_model.predict(X_test_imputed)

# Evaluate XGBoost model
xgb_mse = mean_squared_error(y_test, y_test_pred_xgb)
xgb_mae = mean_absolute_error(y_test, y_test_pred_xgb)
xgb_r2 = r2_score(y_test, y_test_pred_xgb)

# Transformer Model definition and training
class TimeSeriesTransformer(nn.Module):
    def __init__(self, input_dim, model_dim, num_heads, num_layers, output_dim, dropout=0.2):
        super(TimeSeriesTransformer, self).__init__()
        self.model_dim = model_dim
        self.input_embedding = nn.Linear(input_dim, model_dim)
        self.positional_encoding = nn.Parameter(torch.zeros(1, model_dim))
        encoder_layer = nn.TransformerEncoderLayer(d_model=model_dim, nhead=num_heads, dropout=dropout)
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        self.fc_out = nn.Linear(model_dim, output_dim)

    def forward(self, x):
        x = self.input_embedding(x)
        x += self.positional_encoding
        x = self.transformer_encoder(x)
        return self.fc_out(x[:, -1, :])

input_dim = X_train_imputed.shape[1]
model_dim = 128
num_heads = 8
num_layers = 3
output_dim = model_dim
dropout = 0.3

transformer_model = TimeSeriesTransformer(input_dim, model_dim, num_heads, num_layers, output_dim, dropout)
criterion = nn.MSELoss()
optimizer = optim.Adam(transformer_model.parameters(), lr=0.0005)

# Convert data to PyTorch tensors
X_train_tensor = torch.tensor(X_train_imputed, dtype=torch.float32).unsqueeze(1)
y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)
train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)

# Training loop for the improved Transformer
num_epochs = 100
for epoch in range(num_epochs):
    transformer_model.train()
    epoch_loss = 0
    for X_batch, y_batch in train_loader:
        optimizer.zero_grad()
        y_pred = transformer_model(X_batch)
        loss = criterion(y_pred, y_batch)
        loss.backward()
        optimizer.step()
        epoch_loss += loss.item()

# Extract features from the improved Transformer model
transformer_model.eval()
with torch.no_grad():
    train_features = transformer_model(X_train_tensor).numpy()
    test_features = transformer_model(torch.tensor(X_test_imputed, dtype=torch.float32).unsqueeze(1)).numpy()

# XGBoost Model Training with Grid Search for Hyperparameter Tuning
grid_search.fit(train_features, y_train)
best_xgb_model = grid_search.best_estimator_

# Make predictions with the best XGBoost model
y_test_pred_xgb_transformer = best_xgb_model.predict(test_features)

# Evaluate the improved Hybrid model
xgb_transformer_mse = mean_squared_error(y_test, y_test_pred_xgb_transformer)
xgb_transformer_mae = mean_absolute_error(y_test, y_test_pred_xgb_transformer)
xgb_transformer_r2 = r2_score(y_test, y_test_pred_xgb_transformer)

# Final Evaluation Summary
print("Final Model Evaluation Summary:")
print(f"ARIMA Model -> MSE: {arima_mse:.4f}, MAE: {arima_mae:.4f}, R2 Score: {arima_r2:.4f}")
print(f"XGBoost Model -> MSE: {xgb_mse:.4f}, MAE: {xgb_mae:.4f}, R2 Score: {xgb_r2:.4f}")
print(f"Hybrid Transformer + XGBoost Model -> MSE: {xgb_transformer_mse:.4f}, MAE: {xgb_transformer_mae:.4f}, R2 Score: {xgb_transformer_r2:.4f}")

# Optional: Save the best model for future use
import joblib
joblib.dump(best_xgb_model, 'best_xgboost_model.pkl')

# Optional: Save the trained Transformer model
torch.save(transformer_model.state_dict(), 'transformer_model.pth')

print("Models saved successfully.")

! pip install tensorflow numpy scikit-learn

"""## Hybrid Model [LSTM + Transformer]"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, LayerNormalization, MultiHeadAttention, Concatenate, BatchNormalization, GlobalAveragePooling1D
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint

# Load and preprocess the data
def load_and_preprocess_data(file_path):
    # Load the CSV file
    data = pd.read_csv(file_path)

    # Check if the 'Date' column exists and handle it correctly
    if 'Date' in data.columns:
        data['Date'] = pd.to_datetime(data['Date'], errors='coerce')

    # Drop any columns that are irrelevant or contain mostly NaN values
    preprocessed_data = data.dropna(axis=1, how='all')

    # Fill missing numeric values with the column mean
    numeric_columns = preprocessed_data.select_dtypes(include=['float64', 'int64']).columns
    preprocessed_data[numeric_columns] = preprocessed_data[numeric_columns].fillna(preprocessed_data[numeric_columns].mean())

    # Ensure that non-numeric columns are excluded from processing
    preprocessed_data = preprocessed_data.select_dtypes(include=['float64', 'int64'])

    return preprocessed_data

# Prepare data: Convert data to sequences
def create_sequences(data, target_column, seq_length):
    X, y = [], []
    for i in range(len(data) - seq_length):
        X.append(data[i:i + seq_length])
        y.append(data[i + seq_length][target_column])
    return np.array(X), np.array(y)

# Define the improved hybrid LSTM-Transformer model
def build_improved_hybrid_model(seq_length, num_features):
    # LSTM branch
    lstm_input = Input(shape=(seq_length, num_features))
    x1 = LSTM(128, return_sequences=True)(lstm_input)
    x1 = BatchNormalization()(x1)
    x1 = LSTM(128)(x1)
    x1 = BatchNormalization()(x1)

    # Transformer branch
    transformer_input = Input(shape=(seq_length, num_features))
    attention_output = MultiHeadAttention(num_heads=15, key_dim=num_features)(transformer_input, transformer_input)
    attention_output = LayerNormalization(epsilon=1e-6)(attention_output + transformer_input)
    transformer_output = Dense(128, activation='relu')(attention_output)
    transformer_output = GlobalAveragePooling1D()(transformer_output)  # Apply global pooling

    # Combine both branches
    combined = Concatenate()([x1, transformer_output])
    combined = Dense(128, activation='relu')(combined)
    combined = Dropout(0.3)(combined)
    output = Dense(1)(combined)  # Output layer for a single "Close" value

    # Build and compile model
    model = Model(inputs=[lstm_input, transformer_input], outputs=output)
    model.compile(optimizer=Adam(learning_rate=0.0005), loss='mse')

    return model

# Main function to execute the entire process
def main(file_path):
    # Load and preprocess the data
    preprocessed_data = load_and_preprocess_data(file_path)

    # Define sequence length
    SEQ_LENGTH = 9

    # Get the target column index for "Close"
    target_column = preprocessed_data.columns.get_loc("Close")

    # Normalize data for better performance
    data_mean = preprocessed_data.mean(axis=0)
    data_std = preprocessed_data.std(axis=0)
    normalized_data = (preprocessed_data - data_mean) / data_std

    # Create sequences
    X, y = create_sequences(normalized_data.values, target_column, SEQ_LENGTH)

    # Check if there's enough data
    if len(X) < 2:
        raise ValueError("Not enough data to split into training and validation sets.")

    # Manually split into training and validation sets
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

    # Build the improved hybrid model
    model = build_improved_hybrid_model(SEQ_LENGTH, X_train.shape[2])

    # Define callbacks
    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=8, min_lr=1e-7)
    model_checkpoint = ModelCheckpoint('best_model.keras', save_best_only=True, monitor='val_loss', mode='min')

    # Train the model
    history = model.fit([X_train, X_train], y_train,
                        epochs=100,
                        batch_size=16,
                        validation_data=([X_val, X_val], y_val),
                        callbacks=[early_stopping, reduce_lr, model_checkpoint])

    # Evaluate the model on validation data
    val_loss = model.evaluate([X_val, X_val], y_val)
    print(f"Validation Loss: {val_loss}")

    # Load the best model
    model.load_weights('best_model.keras')

    # Use the model for predictions
    predictions = model.predict([X_val, X_val])

    # Normalize the predictions back to original scale
    normalized_predictions = predictions * data_std[target_column] + data_mean[target_column]
    print("First 5 Normalized Predictions for 'Close':\n", normalized_predictions[:5])

    # Extract the actual 'Close' values for the validation set
    actual_values = y_val * data_std[target_column] + data_mean[target_column]
    print("First 5 Actual 'Close' Values:\n", actual_values[:5])

    # Calculate MSE and MAE
    mse = mean_squared_error(actual_values, normalized_predictions)
    mae = mean_absolute_error(actual_values, normalized_predictions)
    print(f"Mean Squared Error (MSE): {mse}")
    print(f"Mean Absolute Error (MAE): {mae}")

if __name__ == "__main__":
    # Provide the path to your CSV file
    file_path = 'Merged_Financial_Data.csv'
    main(file_path)

